---
title: "NLP week 3 - Predictor Model"
author: "Gabor Simon"
date: "2018-02-25"
output:
  html_document:
    toc: true
    theme: united
---

<!-- multi-column layout: https://stackoverflow.com/questions/31753897/2-column-section-in-r-markdown -->
<style>
.column-left{
  float: left;
  width: 33%;
  text-align: left;
}
.column-center{
  display: inline-block;
  width: 33%;
  text-align: center;
}
.column-right{
  float: right;
  width: 33%;
  text-align: right;
}
</style>


## Improvements since last Milestone

(The source of this document, including the all codes are available on [GitHub](https://github.com/gsimon75/NLP_capstone).)

### Regarding concept

1. As we want to predict whole words, not only word stems, it turned out that word stemming is not
   that good idea after all.

2. The role of quotation marks is also revised, because it is not used for actually quoting a different
   passage of text (which would discontinue the current sentence), but as emphasizing expressions
   (which still belong to the current sentence).

3. The handling of various Unicode punctuation marks has been improved, and the definition of 'valid' words got tighter, they:

* must contain at least one lowercase letter (sorry, can't decide what to do with ALL CAPS WORDS)
* may contain only alphanumeric, apostrophe, underscore, plus, dash
* must be at least two characters long, except for 'I' and 'a'
* must begin and end with alphanumeric
* standalone "o'" is replaced with "of", "ol'" with "old"

4. The start-of-sentence and the start-of-sub-sentence positions have their own distributions of words,
   which is different from the global word distribution, so we introduced the special words
   `^` and `,` for them.

5. As we discard the rarest words (for performances sake), we replace them with the `_` symbol, so
   sequences like "The dandelion is..." become  "The _ is ..." and not just "The is ..."

6. The result of the predictor is only the list of the suggested words, in order of preference,
   so strange as it may seem, **it is only the order that matters, the probabilities are irrelevant**.

### Regarding implementation

1. The N-gram data easily exceeds beyond memory capacity (> 1 GB at n >= 4), but counting the
   n-grams can't be serially processed, so we must use some storage backend.
   But storage is slow, so we need some combination of memory and storage (like cached storage or
   swapped memory), and instead of reinventing the wheel we used a database backend (**SQLite**).

2. The task contains much more data processing than mathematical statistics, and here the
   performance of R is inadequate, so I switched to **python** (`rmarkdown` supports it via the
   `reticulate` R package).
   The speed gain is around 60-80 times at text mangling, that is the difference between a
   runtime of half a day and a whole month...

Profiling details are out of scope here, but the main factors in a nutshell:

* R text and list processing itself is about 10 times slower than python
* the `RSQLite` package supports only one simultaneous prepared statement, so for two it means parsing SQL over and over again...
* the `mongolite` package communicates via JSON (in contrast to BSON), so text formatting and parsing impacts here as well
* SQLite is local (in-process), mongo is server-client, so the latter involves some communication that has a cost,
  and we don't really need the benefits of mongo (scalability, aggregate pipes, map-reduce, etc.)
* R pipelines like `f %>% g %>% h` are syntactic sugar for `h(g(f()))` and frees the temporaries only at the end
* native R vector ops like `%in%` are quite fast, but custom functions aren't, and `Vectorize` is just wrapping `lapply()`
* the real strengths of R are math-related, of which we require shamefully little at this project


## The prediction algorithm

If we had only a global list of words ordered by decreasing number of occurences, we could always
suggest the frontmost N of it.

In fact, that is exactly what we will fall back to if we can't predict any better, so we must store
our suggestions only about those contexts that would predict something different than this global one.

We will have a dictionary of (prefix, wordlist) entries, where the prefix means the words we
already read since the beginning of the sentence, and the wordlist contains the words we shall
suggest to the user (in that order).

So, the prediction algorithm goes as follows:

1. First we normalise the prediction context as described (discard invalid characters, add
   leading `^` word, replace rare words with `_`, etc.)

2. When in the middle of a sentence, eg. after the words ^-A-B-C we look up our dictionary for
   the longest possible prefix (that is, ^-A-B-C), if not found, then we look up the shorter
   ones (A-B-C, if not found, then B-C, if not found, then C).

3. If we found nothing, then this prefix don't predict anything unusual, so we use our global
   word list and suggest its frontmost N~sug~ words.

4. If we found a prefix in the dictionary, it means that it differs from the global context,
   and we offer its wordlist as suggestion.

That also means that if we know the maximal number of requested suggestions, we must store
only those top-N suggestions in the wordlists of the dictionary.

**This N~sug~ maximal number of suggestions is a parameter of our model.**

(Practically it doesn't make much sense providing more than eg. 5 suggestions, because it
takes about the same user effort to actually type in a word as choosing it from a list longer
than this.)



## The dictionary builder algorithm

0. Split the input to training and test parts, the current ratio is 80% vs. 20%

1. Normalise the input lines and split them to sentences.

2. Collect the global list of words, ordered by descending number of occurences.
   For cutting away the irrelevant words as soon as possible, we discard the least frequent 10%.
   The remaining ones will be the default suggestion for those cases that aren't in our dictionary.

   **This p~discard~ rare word removal limit is a parameter of our model**

3. Filter the sentences for 'rare' words, replace them with `_`

4. Split the sentences into words, form N-grams from them, and count their
   occurences through all sentences.

5. Update the dictionary by the N-gram distribution (see below)

6. Increment N and repeat until we got enough information


### Processing an N-gram distribution

The question here is whether we need to store a given N-gram plus its possible follower words or not.

Storing uses resources, so the idea is to do it only if it gives us information.

If the suggestion algorithm can't find a given prefix, it'll revert to its parent (one shorter),
so **we need to store a given N-gram prefix only if its follower distribution differs from its parent**.

How much improvement would then a prefix give us over its parent?

Suppose the 'A-B-C' prefix occurs N~A,B,C~ times and would suggest the words 'P, Q, R', and its parent, the
'B-C' prefix would suggest the words 'P, S, R'.

Then the improvement of storing this 'A-B-C-{P, Q, R}' would bring is the distance between the two
suggestion sequences 'P, Q, R' and 'P, S, R', times its occurence N~A,B,C~.

For telling the distance between suggestion word sequences we use a
[Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance#Definition)
metric, only applying it to words instead of letters.


### Maintaining the dictionary

If we just store all such prefix+suggestion entries, the dictionary would grow to impractical sizes, so
we defined a constraint:

We store only the most valuable (prefix, suggestions) combinations,
and this **N~dict~ maximal number of entries of our dictionary is also a parameter of our model**.

For the definition of 'value' we use the already mentioned measure of improvement: the suggestion distance
from the parent times the number of occurences.

So, adding a new entry to the dictionary goes like:

1. If the dict is not full, then we add the (prefix, suggestions, value) triplet.
2. If the dict is full, we look up the least valuable entry in it, and
3. If the new entry is less valuable than the (already stored) least valuable one, then we just discard the new one
4. Otherwise add the new one, but discard the least valuable one
5. When not adding a new entry, or discarding an already stored one, we effectively 
   unify it with its parent, so we must add its 'value' to its parents 'value' field

## Data representation

As discussed, we use SQLite databases, and the plural is intentional here.

On one hand, the dictionary will contain our model, it is our deliverable item, so we must be able to ship it.  
On the other hand, the N-gram collection is needed only during the training phase, so it must be separate
from the dictionary.  
On the gripping hand :), the raw input data is plain text file, so we will have at
least these three types of storage.

### Plaintext files

* `final/<locale>/<locale>.<source>.txt`: original input data
* `final/<locale>/<locale>.<source>.train.txt`: approx. 80% of the input for training the model
* `final/<locale>/<locale>.<source>.test.txt`: the rest of the input for testing the model, eg.:

  ```In the years thereafter, most of the Oil fields and platforms were named after pagan “gods”. ```

* `final/<locale>/<locale>.<source>.all.sentences`: the training data normalised and split to sentences, eg.:

  ```In the years thereafter , most of the Oil fields and platforms were named after pagan gods```

* `final/<locale>/<locale>.<source>.common.sentences`: the sentences, with the rare words replaced with `_`, `^` added, eg.:

  ```^ In the years _ , most of the Oil fields and _ were named after _ gods```

### Database files
* `final/<locale>/<locale>.<source>.<N>.db`: the N-gram statistics, (prefix, follower, count)
* `dict.db`: the dictionary, (prefix, suggestions, value)

This latter `dict.db` also contains the word statistics of the trained corpora:
the word, the term frequency, the number of occurences and the coverage.

(Coverage means the cumulative sum of term frequencies up to that word in the
list ordered by descending number of occurences.)

## Implementation - training the model


```{python test_python_code}
for i in range(1, 10):
	print i
```



## Current milestone status

At this stage the following steps have been completed:
 
* ...


## Next steps

Next we will implement the n-gram generation using `tokenizer::tokenize_ngrams(...)`,
and the measurement of the information gain.

As it involves manipulating large quantities of data, the data representation
may require changes as well.

