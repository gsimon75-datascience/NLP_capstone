---
title: "NLP week 1 - Milestone Report"
author: "Gabor Simon"
date: "2018-02-18"
output: html
---

<!-- multi-column layout: https://stackoverflow.com/questions/31753897/2-column-section-in-r-markdown -->
<style>
.column-left{
  float: left;
  width: 33%;
  text-align: left;
}
.column-center{
  display: inline-block;
  width: 33%;
  text-align: center;
}
.column-right{
  float: right;
  width: 33%;
  text-align: right;
}
</style>

## The concept

Our Word Predictor project will employ a statistical model based on the fact that the words don't follow each other
in any arbitrary order (at least not with expressing some meaning), and **the probability
of a word at a given position can be estimated by considering the other words preceding it**.
For example, the word 'shuttle' is more likely to occur after the word 'space' or after 'airport', than
after other words.

This idea would produce kind of a dictionary that tells us that after a given pre-text which words
are expected to follow with what probability.

Naturally, this look-behind doesn't have to be restricted to one preceding word, longer pre-texts
describe the situation more precisely, but it comes for a price: the dictionary will grow very quickly.

Therefore it is essential that we can decide how big pre-texts are worth considering in each case,
and don't grow them when we wouldn't gain significant improvement by it.

The source of this document, including the R codes are available on [GitHub](https://github.com/gsimon75/NLP_capstone).

## The input corpora

For training our model we use 3 text corpora: one from blog comments, one from newsfeeds and one from tweets.

```{r libs, results='hide', echo=F, message=F, warning=F}
library(tibble)
library(dplyr)
library(stringr)
library(tokenizers)
library(lattice)
```

```{r raw_stats, echo=F}
locale <- "en_US"
sources <- c("blogs", "news", "twitter")

basename <- function(locale, src) {
	paste0("final/", locale, "/", locale, ".", src)
}

print_file_stats <- function(basename, suffix) {
	input_filename <- paste0(basename, ".", suffix)
	input <- file(input_filename, open="r")
	lines_per_block <- 10000
	total_stats <- tibble::as_tibble()
	while (T) {
		lines <- readLines(con=input, n=lines_per_block, encoding="UTF-8", skipNul=T)
		num_lines = length(lines)
		if (num_lines <= 0) break

		this_stat <- lines %>% trimws() %>% tibble::as_tibble() %>%
			transmute(line_len=nchar(value), num_words=str_count(value, "\\S+")) %>%
			group_by() %>% summarize(num_chars=sum(line_len), num_words=sum(num_words), num_lines=n(), max_line_len=max(line_len))

		total_stats <- total_stats %>% rbind(this_stat) %>% summarize_all(sum)
	}
	close(input)
	message("File stats; file='", input_filename,
		"', lines='", total_stats$num_lines,
		"', words='", total_stats$num_words,
		"', chars='" , total_stats$num_chars,
		"', max_line_len='", total_stats$max_line_len, "'")
}

for (src in sources) {
	print_file_stats(basename(locale, src), "txt")
}
```

## Basic pre-processing

The raw data must be pre-processed even for an exploratory analysis. The steps we do here
will require further refinement when we have gained some detailed information,
but for a start they'll do.

When considering the sequences of words, it only makes sense to process words that
semantically belong together, we can't really predict the start of one sentence from
the end of the previous one.

Similarly, we can't predict a part of a compound sentence from the previous part either,
so we must first split each entry to such sub-sequence along the usual punctuations.

Then, to reduce the task we can coalesce all forms of the same word into one stem,
so for example all of 'types', 'typed', 'typing' becomes simply 'typ'.


## Progressive filtering

Even after pre-processing, generating all 3-grams or 4-grams still would be overwhelming, so we
will need some progressive filtering sooner or later.


### By frequency

If a word occurs only 5 times by itself, then it can occur only at most in 10 word-pairs
(5 places at the beginning, 5 places at the end).

If a word pair occurs only 5 times, then it can occur only at most in 10 word-triplets
(5 places at the beginning, 5 places at the end).

As a general rule:
*If an N-gram is insignificantly rare, then so will be all (N+1)-grams that contain it.*

Therefore we count the words (1-grams) first, then discard the rarest ones (say, up to 5% of
all occurences), then we need to collect only those 2-grams that are based on the remaining
words, and so on.


### By gained information

Suppose we have an 3-gram 'A-B-C', and we collect a statistics on what '-D' words follow
it with what probability.

Then we collect the similar statistics for only its trailing 2-gram 'B-C'.

If the probabilities after 'A-B-C' are roughly the same as after only 'B-C', then there
is no point in extending it to 3-gram with 'A-'.

*If the distribution of the followers of an N-gram are close to the followers of its
trailing (N-1)-gram, then this N-gram brings no new information, and can be discarded.*

As this will require comparing distributions (against some p-value limit, for example)
by each N-gram, this step will require the most processing capacity.


## Current milestone status

At this stage the following pre-processing steps have been completed:
 
* splitting the input to sub-sentences
* reducing the words to their stems
* collecting the statistics of the words


## Exploratory analysis

```{r preprocessing, echo=F}
collect_subsentences <- function(input_basename) {
	input_filename <- paste0(input_basename, ".txt")
	output_filename <- paste0(input_basename, ".subs")
	words_filename <- paste0(input_basename, ".words.rds")

	all_words <- tibble::as_tibble()
	if (!file.exists(output_filename)) {

		input <- file(input_filename, open="r")
		output <- file(output_filename, open="w")
		lines_per_block <- 10000

		message("Collecting subsentences; input='", input_filename, "', output='", output_filename, "'")
		total_lines <- 0
		while (T) {
			lines <- readLines(con=input, n=lines_per_block, encoding="UTF-8", skipNul=T)
			num_lines = length(lines)
			if (num_lines <= 0) break

			lines <- lines %>%
			gsub(pattern="[ \t\u2000-\u200d\u2060\u202f\u205f]+", replacement=" ") %>%
			# here: whitespaces are coalesced into a single space
			gsub(pattern="[.,!?()\u2010-\u205e]", replacement="\n") %>%
			# here: sub-sentence delimiters are replaced with line break marks
			gsub(pattern="^\n+", replacement="") %>% gsub(pattern="\n+$", replacement="") %>% strsplit("\n+") %>% unlist(recursive=F) %>%
			# here: sub-sentences are split apart
			tokenize_word_stems(language="english")
			# here: words are replaced by list of their stems

			# count the words
			these_words <- lines %>% unlist() %>% tibble::as_tibble() %>% transmute(word=value) %>% group_by(word) %>% summarize(n=n())
			all_words <- all_words %>% rbind(these_words) %>% group_by(word) %>% summarize(n=sum(n))

			# write the output
			lines %>% sapply(function(x) {paste0(x,collapse=" ")}) %>% writeLines(con=output)

			total_lines <- total_lines + num_lines
			message("  Processed block; lines='", total_lines, "'")
		}
		close(output)
		close(input)

		saveRDS(all_words, words_filename)
	} else {
		all_words <- readRDS(words_filename)
	}
	total <- sum(all_words$n)
	all_words %>% arrange(desc(n)) %>% mutate(coverage=cumsum(n)/total)
}

words_blogs <- collect_subsentences(basename(locale, "blogs")) %>% mutate(src="blogs")
words_news <- collect_subsentences(basename(locale, "news")) %>% mutate(src="news")
words_twitter <- collect_subsentences(basename(locale, "twitter")) %>% mutate(src="twitter")

plot_by_frequency <- function(pct, fm) {
	frequent_blogs <- words_blogs %>% filter(coverage <= pct)
	frequent_blogs$idx <- seq.int(nrow(frequent_blogs))
	frequent_news <- words_news %>% filter(coverage <= pct)
	frequent_news$idx <- seq.int(nrow(frequent_news))
	frequent_twitter <- words_twitter %>% filter(coverage <= pct)
	frequent_twitter$idx <- seq.int(nrow(frequent_twitter))

	all_words <- rbind(frequent_blogs, frequent_news, frequent_twitter)
	xyplot(as.formula(fm), data=all_words, type="l", layout=c(3, 1))
}
```

The distribution of the most frequent 90% of words in the corpora shows that
these top words are unproportionally frequent.

```{r plot_occurences, echo=F}
plot_by_frequency(pct=0.9, n ~ idx | src)
```

The coverage plot seems to confirm the hypothesis that

* the top 300 words cover a basic level of knowledge (~50%)
* the top 1000 words cover an advanced level (~80%)
* about 10000 words are required for a native level (>95%)

```{r plot_coverage, echo=F}
plot_by_frequency(pct=0.9, coverage ~ idx | src)
```

The most frequent words seem to correlate between the sources:

<div class="column-left">
Blogs
```{r coverage_blogs, echo=F}
words_blogs %>% mutate(src=NULL) %>% print(n=15)
```
</div>

<div class="column-center">
News
```{r coverage_news, echo=F}
words_news %>% mutate(src=NULL) %>% print(n=15)
```
</div>

<div class="column-right">
Twitter
```{r coverage_twitter, echo=F}
words_twitter %>% mutate(src=NULL) %>% print(n=15)
```
</div>

Fortunately the information-gain-based filtering will eliminate most of the rare words
(including the proper nouns, for example), so our final dictionary won't be
impractically large.

## Next steps

Next we will implement the n-gram generation using `tokenizer::tokenize_ngrams(...)`,
and the measurement of the information gain.

As it involves manipulating large quantities of data, the data representation
may require changes as well.

